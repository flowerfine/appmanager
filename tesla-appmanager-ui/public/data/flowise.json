{
  "nodes": [
    {
      "id": "a1",
      "shape": "flowise-node",
      "x": 0,
      "y": 0,
      "ports": [
        {
          "id": "a1-out",
          "group": "out"
        }
      ],
      "data": {
        "label": "Prompt Template",
        "name": "promptTemplate",
        "version": 1,
        "description": "Schema to represent a basic prompt for an LLM",
        "type": "PromptTemplate",
        "icon": "/nodes/flowise/prompt.svg",
        "category": "Prompts",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Template",
            "name": "template",
            "type": "string",
            "rows": 4,
            "placeholder": "What is a good name for a company that makes {product}?",
            "id": "promptTemplate_0-input-template-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "promptTemplate_0-input-promptValues-json"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "template": "{query}",
          "promptValues": "{\"query\":\"{{question}}\"}"
        },
        "outputAnchors": [
          {
            "id": "promptTemplate_0-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate",
            "name": "promptTemplate",
            "label": "PromptTemplate",
            "type": "PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate"
          }
        ],
        "outputs": {}
      }
    },
    {
      "id": "a2",
      "shape": "flowise-node",
      "x": 300,
      "y": 0,
      "ports": [
        {
          "id": "a2-in",
          "group": "in"
        },
        {
          "id": "a2-out",
          "group": "out"
        }
      ],
      "data": {
        "label": "Prompt Template",
        "name": "promptTemplate",
        "description": "Schema to represent a basic prompt for an LLM",
        "version": 1,
        "type": "PromptTemplate",
        "icon": "/nodes/flowise/prompt.svg",
        "category": "Prompts",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Template",
            "name": "template",
            "type": "string",
            "rows": 4,
            "placeholder": "What is a good name for a company that makes {product}?",
            "id": "promptTemplate_1-input-template-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "promptTemplate_1-input-promptValues-json"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "template": "Reply with nothing else but the following:\n![]({text})",
          "promptValues": "{\"text\":\"{{llmChain_0.data.instance}}\"}"
        },
        "outputAnchors": [
          {
            "id": "promptTemplate_1-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate|Runnable",
            "name": "promptTemplate",
            "label": "PromptTemplate",
            "type": "PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate | Runnable"
          }
        ],
        "outputs": {}
      }
    },
    {
      "id": "a3",
      "shape": "flowise-node",
      "x": 600,
      "y": 0,
      "ports": [
        {
          "id": "a3-in",
          "group": "in"
        },
        {
          "id": "a3-out",
          "group": "out"
        }
      ],
      "data": {
        "label": "Replicate",
        "name": "replicate",
        "version": 2,
        "description": "Use Replicate to run open source models on cloud",
        "type": "Replicate",
        "icon": "/nodes/flowise/replicate.svg",
        "category": "LLMs",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "replicateApi"
            ],
            "id": "replicate_0-input-credential-credential"
          },
          {
            "label": "Model",
            "name": "model",
            "type": "string",
            "placeholder": "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5",
            "optional": true,
            "id": "replicate_0-input-model-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "description": "Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value.",
            "default": 0.7,
            "optional": true,
            "id": "replicate_0-input-temperature-number"
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "description": "Maximum number of tokens to generate. A word is generally 2-3 tokens",
            "optional": true,
            "additionalParams": true,
            "id": "replicate_0-input-maxTokens-number"
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "step": 0.1,
            "description": "When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens",
            "optional": true,
            "additionalParams": true,
            "id": "replicate_0-input-topP-number"
          },
          {
            "label": "Repetition Penalty",
            "name": "repetitionPenalty",
            "type": "number",
            "step": 0.1,
            "description": "Penalty for repeated words in generated text; 1 is no penalty, values greater than 1 discourage repetition, less than 1 encourage it. (minimum: 0.01; maximum: 5)",
            "optional": true,
            "additionalParams": true,
            "id": "replicate_0-input-repetitionPenalty-number"
          },
          {
            "label": "Additional Inputs",
            "name": "additionalInputs",
            "type": "json",
            "description": "Each model has different parameters, refer to the specific model accepted inputs. For example: <a target=\"_blank\" href=\"https://replicate.com/a16z-infra/llama13b-v2-chat/api#inputs\">llama13b-v2</a>",
            "additionalParams": true,
            "optional": true,
            "id": "replicate_0-input-additionalInputs-json"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "replicate_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "model": "stability-ai/sdxl:af1a68a271597604546c09c64aabcd7782c114a63539a4a8d14d1eeda5630c33",
          "temperature": 0.7,
          "maxTokens": "",
          "topP": "",
          "repetitionPenalty": "",
          "additionalInputs": ""
        },
        "outputAnchors": [
          {
            "id": "replicate_0-output-replicate-Replicate|BaseChatModel|LLM|BaseLLM|BaseLanguageModel|Runnable",
            "name": "replicate",
            "label": "Replicate",
            "type": "Replicate | BaseChatModel | LLM | BaseLLM | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {}
      }
    },
    {
      "id": "a4",
      "shape": "flowise-node",
      "x": 900,
      "y": 0,
      "ports": [
        {
          "id": "a4-in",
          "group": "in"
        },
        {
          "id": "a4-out",
          "group": "out"
        }
      ],
      "data": {
        "label": "LLM Chain",
        "name": "llmChain",
        "version": 3,
        "description": "Chain to run queries against LLMs",
        "type": "LLMChain",
        "icon": "/nodes/flowise/llm_chain.svg",
        "category": "Chains",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Chain Name",
            "name": "chainName",
            "type": "string",
            "placeholder": "Name Your Chain",
            "optional": true,
            "id": "llmChain_0-input-chainName-string"
          }
        ],
        "inputAnchors": [
          {
            "label": "Language Model",
            "name": "model",
            "type": "BaseLanguageModel",
            "id": "llmChain_0-input-model-BaseLanguageModel"
          },
          {
            "label": "Prompt",
            "name": "prompt",
            "type": "BasePromptTemplate",
            "id": "llmChain_0-input-prompt-BasePromptTemplate"
          },
          {
            "label": "Output Parser",
            "name": "outputParser",
            "type": "BaseLLMOutputParser",
            "optional": true,
            "id": "llmChain_0-input-outputParser-BaseLLMOutputParser"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "llmChain_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "model": "{{replicate_0.data.instance}}",
          "prompt": "{{promptTemplate_0.data.instance}}",
          "outputParser": "",
          "chainName": "",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "options": [
              {
                "id": "llmChain_0-output-llmChain-LLMChain|BaseChain|Runnable",
                "name": "llmChain",
                "label": "LLM Chain",
                "type": "LLMChain | BaseChain | Runnable"
              },
              {
                "id": "llmChain_0-output-outputPrediction-string|json",
                "name": "outputPrediction",
                "label": "Output Prediction",
                "type": "string | json"
              }
            ],
            "default": "llmChain"
          }
        ],
        "outputs": {
          "output": "outputPrediction"
        }
      }
    },
    {
      "id": "a5",
      "shape": "flowise-node",
      "x": 1200,
      "y": 0,
      "ports": [
        {
          "id": "a5-in",
          "group": "in"
        },
        {
          "id": "a5-out",
          "group": "out"
        }
      ],
      "data": {
        "label": "LLM Chain",
        "name": "llmChain",
        "version": 3,
        "description": "Chain to run queries against LLMs",
        "type": "LLMChain",
        "icon": "/nodes/flowise/llm_chain.svg",
        "category": "Chains",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Chain Name",
            "name": "chainName",
            "type": "string",
            "placeholder": "Name Your Chain",
            "optional": true,
            "id": "llmChain_1-input-chainName-string"
          }
        ],
        "inputAnchors": [
          {
            "label": "Language Model",
            "name": "model",
            "type": "BaseLanguageModel",
            "id": "llmChain_1-input-model-BaseLanguageModel"
          },
          {
            "label": "Prompt",
            "name": "prompt",
            "type": "BasePromptTemplate",
            "id": "llmChain_1-input-prompt-BasePromptTemplate"
          },
          {
            "label": "Output Parser",
            "name": "outputParser",
            "type": "BaseLLMOutputParser",
            "optional": true,
            "id": "llmChain_1-input-outputParser-BaseLLMOutputParser"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "llmChain_1-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "model": "{{chatOpenAI_0.data.instance}}",
          "prompt": "{{promptTemplate_1.data.instance}}",
          "outputParser": "",
          "chainName": "",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "options": [
              {
                "id": "llmChain_1-output-llmChain-LLMChain|BaseChain|Runnable",
                "name": "llmChain",
                "label": "LLM Chain",
                "type": "LLMChain | BaseChain | Runnable"
              },
              {
                "id": "llmChain_1-output-outputPrediction-string|json",
                "name": "outputPrediction",
                "label": "Output Prediction",
                "type": "string | json"
              }
            ],
            "default": "llmChain"
          }
        ],
        "outputs": {
          "output": "llmChain"
        }
      }
    },
    {
      "id": "a6",
      "shape": "flowise-node",
      "x": 1500,
      "y": 0,
      "ports": [
        {
          "id": "a6-in",
          "group": "in"
        }
      ],
      "data": {
        "label": "ChatOpenAI",
        "name": "chatOpenAI",
        "version": 6,
        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
        "type": "ChatOpenAI",
        "icon": "/nodes/flowise/openai.svg",
        "category": "Chat Models",
        "author": "flowise",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "openAIApi"
            ],
            "id": "chatOpenAI_0-input-credential-credential"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "gpt-3.5-turbo",
            "id": "chatOpenAI_0-input-modelName-options"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOpenAI_0-input-temperature-number"
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-maxTokens-number"
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-topP-number"
          },
          {
            "label": "Frequency Penalty",
            "name": "frequencyPenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-frequencyPenalty-number"
          },
          {
            "label": "Presence Penalty",
            "name": "presencePenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-presencePenalty-number"
          },
          {
            "label": "Timeout",
            "name": "timeout",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-timeout-number"
          },
          {
            "label": "BasePath",
            "name": "basepath",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-basepath-string"
          },
          {
            "label": "BaseOptions",
            "name": "baseOptions",
            "type": "json",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-baseOptions-json"
          },
          {
            "label": "Allow Image Uploads",
            "name": "allowImageUploads",
            "type": "boolean",
            "description": "Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, and Conversational Agent",
            "default": false,
            "optional": true,
            "id": "chatOpenAI_0-input-allowImageUploads-boolean"
          },
          {
            "label": "Image Resolution",
            "description": "This parameter controls the resolution in which the model views the image.",
            "name": "imageResolution",
            "type": "options",
            "options": [
              {
                "label": "Low",
                "name": "low"
              },
              {
                "label": "High",
                "name": "high"
              },
              {
                "label": "Auto",
                "name": "auto"
              }
            ],
            "default": "low",
            "optional": false,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-imageResolution-options"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOpenAI_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "gpt-3.5-turbo",
          "temperature": "0",
          "maxTokens": "",
          "topP": "",
          "frequencyPenalty": "",
          "presencePenalty": "",
          "timeout": "",
          "basepath": "",
          "baseOptions": "",
          "allowImageUploads": false,
          "imageResolution": "low"
        },
        "outputAnchors": [
          {
            "id": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOpenAI",
            "label": "ChatOpenAI",
            "type": "ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {}
      }
    }
  ],
  "edges": [
    {
      "id": "edge-1",
      "source": {
        "cell": "a1",
        "port": "a1-out"
      },
      "target": {
        "cell": "a2",
        "port": "a2-in"
      },
      "shape": "flowise-curve",
      "zIndex": -1,
      "data": {
        "source": "a1",
        "target": "a2"
      }
    },
    {
      "id": "edge-2",
      "source": {
        "cell": "a2",
        "port": "a2-out"
      },
      "target": {
        "cell": "a3",
        "port": "a3-in"
      },
      "shape": "flowise-curve",
      "zIndex": -1,
      "data": {
        "source": "a2",
        "target": "a3"
      }
    },
    {
      "id": "edge-3",
      "source": {
        "cell": "a3",
        "port": "a3-out"
      },
      "target": {
        "cell": "a4",
        "port": "a4-in"
      },
      "shape": "flowise-curve",
      "zIndex": -1,
      "data": {
        "source": "a3",
        "target": "a4"
      }
    },
    {
      "id": "edge-4",
      "source": {
        "cell": "a4",
        "port": "a4-out"
      },
      "target": {
        "cell": "a5",
        "port": "a5-in"
      },
      "shape": "flowise-curve",
      "zIndex": -1,
      "data": {
        "source": "a4",
        "target": "a5"
      }
    },
    {
      "id": "edge-5",
      "source": {
        "cell": "a5",
        "port": "a5-out"
      },
      "target": {
        "cell": "a6",
        "port": "a6-in"
      },
      "shape": "flowise-curve",
      "zIndex": -1,
      "data": {
        "source": "a5",
        "target": "a6"
      }
    }
  ]
}
